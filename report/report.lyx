#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3.5cm
\rightmargin 3cm
\bottommargin 3.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand include
filename "math_shorthand.lyx"

\end_inset


\end_layout

\begin_layout Title
RLOP: RL Methods in Option Pricing from a Mathematical Perspective
\end_layout

\begin_layout Author
Ziheng Chen
\begin_inset Foot
status open

\begin_layout Plain Layout
eid: zc5282, email: 
\begin_inset CommandInset href
LatexCommand href
name "stokes615@utexas.edu"
type "mailto:"
literal "true"

\end_inset


\begin_inset Newline newline
\end_inset

We thank Zhou Fang for his initial motivation and helpful discussion on
 the financial aspects.
\begin_inset Newline newline
\end_inset

A 5-minute introduction can be found at:
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
In this work, we build two environments, namely the modified QLBS and RLOP
 models, from a mathematics perspective which enables RL methods in option
 pricing through replicating by portfolio.
 We implement the environment specifications, the learning algorithm, and
 agent parametrization by a neural network.
 The learned optimal hedging strategy is compared against the BS prediction.
 The effect of various factors is considered and studied based on how they
 affect the optimal price and position.
\end_layout

\begin_layout Section
Introduction and Background
\end_layout

\begin_layout Standard
Option pricing is one of the central and open questions in the mathematical
 finance field.
 It motivates the advancement of the stochastic process theory and becomes
 one of the practical applications of sophisticated theories.
 To begin with, options are a type of derivative that gives the holder the
 right, but not the obligation, to buy or sell the underlying asset at a
 specified price (a.k.a.
 the strike price) within a specific period.
 The name `derivative' implies that those securities are derived from the
 origin asset.
 Although investors can long and short options for seeking risky profits,
 one primary feature of options is to obtain a certain share of assets with
 a fixed amount of payment in the future.
 This helps to control volatility and uncertainty.
 Thus, the market maker who sells the option charges the buyer a certain
 amount of fee that amounts to the expected risk.
\end_layout

\begin_layout Standard
We focus on the European call in this paper.
 One can only exercise a European option at the terminal time and a call
 option gives the holder the right to buy.
 The holder usually exercises the European call if the asset price ends
 up above the strike price and takes no action otherwise.
 If the market is liquid enough, the holder can sell the asset immediately
 after obtaining it, making a marginal profit.
 Therefore, we define the payoff of a European call as
\begin_inset Formula 
\begin{equation}
\text{payoff}=\max\left\{ S-K,0\right\} \label{eq:intro-payoff}
\end{equation}

\end_inset

where 
\begin_inset Formula $S$
\end_inset

 is the asset price at the exercise time and 
\begin_inset Formula $K$
\end_inset

 stands for the strike price.
 Although it is tempting to price the option based on this payoff function
 with consideration of each path probability, this is usually not the best
 way since the option is designed to measure the risk of obtaining the asset.
 If there is a general trend in the price process, one can hold a consistent
 position in the asset and purchase an option in addition.
 Thus, the more popular approach is to manage a portfolio that replicates
 the option, i.e.
 the holder invests the same amount of money as the price of the option
 in a replication portfolio and gets the same amount of payoff (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:intro-payoff"

\end_inset

) at the terminal time.
 The portfolio borrows some extra funds from an external risk-free source
 to hold a long position of the underlying asset and adjusts its position
 according to the change in the asset price.
 This approach is referred to as the replication by portfolio method.
\end_layout

\begin_layout Standard
We conduct a brief review in this section, starting by introducing the Black-Sch
oles model which solves the optimal hedging strategy and the pricing problem
 at the same time, moving to a literature review where multiple types of
 asset pricing are involved, and discussing how to model transaction costs.
\end_layout

\begin_layout Subsection
Black-Scholes Model
\end_layout

\begin_layout Standard
The Black-Scholes formula (or BS formula or short) is the first formula
 that prices the European-style call option.
 There are two equivalent ways of calculating the price, either by solving
 the Black-Scholes-Merton equation or use the risk-neutral measure.
 The price of an European call of time span 
\begin_inset Formula $T$
\end_inset

 and a strike price 
\begin_inset Formula $K$
\end_inset

 is solved by 
\begin_inset Formula 
\begin{equation}
c_{\text{BS}}\left(t,x\right)=xN\left(d_{+}\left(T-t,x\right)\right)-Ke^{-r\left(T-t\right)}N\left(d_{-}\left(T-t,x\right)\right)\label{eq:bs-price}
\end{equation}

\end_inset

where 
\begin_inset Formula $0\le t<T$
\end_inset

 is the current time and 
\begin_inset Formula $x$
\end_inset

 is the current asset price, with 
\begin_inset Formula $r$
\end_inset

 being the risk-free interest rate, 
\begin_inset Formula $\sigma$
\end_inset

 being the volatility, 
\begin_inset Formula $N\left(y\right):=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}\exp\left(-\frac{z^{2}}{2}\right)\d z$
\end_inset

 being the cumulative standard normal distribution, and 
\begin_inset Formula $d_{\pm}$
\end_inset

 defined as
\begin_inset Formula 
\[
d_{\pm}\left(\tau,x\right):=\frac{1}{\sigma\sqrt{\tau}}\left[\log\frac{x}{K}+\left(r\pm\frac{\sigma^{2}}{2}\right)\tau\right].
\]

\end_inset

The optimal hedging position can be derived as the partial derivative of
 Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bs-price"

\end_inset

 as 
\begin_inset Formula 
\begin{equation}
u_{\text{BS}}\left(t,x\right)=N\left(d_{+}\left(T-t,x\right)\right).\label{eq:bs-hedge}
\end{equation}

\end_inset

We leave the details to 
\begin_inset CommandInset citation
LatexCommand cite
key "shreve2004stochastic,alma991057973644906011"
literal "false"

\end_inset

 for more in-depth theory and computation.
 
\end_layout

\begin_layout Standard
This set of theory surpassed most of the option pricing theories when it
 was first published and is still helpful from since.
 With that being said, the theory relies on a critical assumption that the
 agent is able to continuously rehedge the positions of the replication
 portfolio.
 This is either impractical or costly to most of the investing parties.
 In the view of stochastic integration, we use a Riemann-style sum with
 left endpoints to approximate the Ito integral.
 The accumulation of the temporal discretization errors will affect the
 final payoff profoundly.
\end_layout

\begin_layout Subsection
Literature Review
\end_layout

\begin_layout Standard
There have been vast works and literatures on option pricing.
 A few exercising strategies for American option are reviewed in 
\begin_inset CommandInset citation
LatexCommand cite
key "li2009learning"
literal "false"

\end_inset

.
 The state space is composed of asset price trajectories 
\begin_inset Formula $\left(S_{0},S_{1},\dots,S_{T}\right)$
\end_inset

 and an absorbing state 
\begin_inset Formula $\boldsymbol{e}$
\end_inset

 which serves as the destination after the option has been exercised.
 The action space simply contains two actions: hold and exercise.
 The only non-zero reward is given when the option has been exercised at
 the value specified by the option.
 The Q-function is derived as in classical RL problems and it is the quantity
 of interest.
 The first method, LSPI (LS policy iteration), combines LSTD (LS with TD
 update) and policy iteration together to achieve efficient learning.
 The second and third ones, FQI (fitted Q-iteration algorithm) and LSMC
 (Least squares Monte Carlo), take a DP approach where the exact exercising
 problem is solved at each trading moment; the only difference lies in that
 FDI takes a forward view in time while LSMC is backward, starting from
 the exercising time.
 In the same spirit, 
\begin_inset CommandInset citation
LatexCommand cite
key "fathan2021deep"
literal "false"

\end_inset

 investigates the efficiency and effectiveness of different parametrizaiton
 structures.
 Three algorithms, namely double deep Q-Learning (DDQN), categorical distributio
nal RL (C51), and implicit quantile networks (IQN) are compared where DDQN
 learns the optimal Q-value function while the other two algorithms try
 to learn the full distribution of the discounted reward.
 They are tested on empirical data as well as simulated geometric Brownian
 motion trajectories.
\end_layout

\begin_layout Standard
Different RL architectures can be deployed in this field as well.
 The Actor-Critic structure is used in 
\begin_inset CommandInset citation
LatexCommand cite
key "marzban2021deep"
literal "false"

\end_inset

 for Equal Risk Pricing (ERP) in a risk averse setting under the framework
 studied in 
\begin_inset CommandInset citation
LatexCommand cite
key "tamar2015policy"
literal "false"

\end_inset

.
 The concept of 
\begin_inset Formula $\tau$
\end_inset

-expectile 
\begin_inset Formula 
\[
\overline{\rho}\left(X\right)=\arg\min_{q}\tau\bE\left[\left(q-X\right)_{+}^{2}\right]+\left(1-\tau\right)\bE\left[\left(q-X\right)_{-}^{2}\right]
\]

\end_inset

is used to elicit a coherent risk measure.
 The value function is defined as the portfolio value under the recursive
 coherent risk measure realized by expectiles, i.e.
\begin_inset Formula 
\[
V_{t}\left(S_{t},Y_{t}\right)=\inf_{\xi_{t}}\overline{\rho}\left(-\xi_{t}^{T}\Delta S_{t+1}+V_{t+1}\left(S_{t+1},Y_{t+1}\right)|S_{t},Y_{t}\right)
\]

\end_inset

with terminal condition 
\begin_inset Formula $V_{T}\left(S_{T},Y_{T}\right)=F\left(S_{T},Y_{T}\right)$
\end_inset

 specified by the option contract.
 With that being established, we can apply the policy gradient method where
 the critic network updates the value estimate which the actor network can
 refer to and build its policy upon.
 The network uses a classical fully multilayer structure with alternating
 activation functions.
\end_layout

\begin_layout Standard
A hybrid attempt is carried out in 
\begin_inset CommandInset citation
LatexCommand cite
key "grassl2010reinforcement"
literal "false"

\end_inset

 where the pricing strategy is based on a combination of optimal stopping
 and terminal payoff.
 The idea is that the agent can either hold the derivative until the terminal
 time, executing the contract to get the payoff written, or sell the derivative
 earlier according to the price at that particular moment.
 The reward function is thus simplified defined as the selling/execution
 price if such scenario happens.
 The value function is parameterized by kernel function approximation and
 the algorithm is tested using simulated geometric Brownian paths of an
 European call option.
\end_layout

\begin_layout Standard
We point out that it is also possible to directly solve the HJB equation
 if the associated RL problem is formulated as a control porblem.
 We refer to 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2021distributional"
literal "false"

\end_inset

 for more details on distributional offline continuous-time RL learning
 algorithms.
\end_layout

\begin_layout Subsection
Trading Cost 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Trading-Cost"

\end_inset


\end_layout

\begin_layout Standard
One other important factor in option pricing is the trading cost (or transaction
 cost, used interchangeably).
 The trading cost occurs when the hedging position of the replication portfolio
 changes, making a portion of the invested capital unavailable.
 Thus, the presence of trading cost increases the price of the option in
 general.
\end_layout

\begin_layout Standard
A way of understanding and modeling the trading cost is to use the concept
 of bid-ask spread.
 Consider the limit order book of a particular asset at a given time.
 Empirical observation shows that the selling (a.k.a.
 ask) orders are always above the buying orders (a.k.a.
 bid) orders since one wishes to buy low and sell high.
 Moreover, the orders are not uniformly distributed: there are fewer orders
 close to the mid price (i.e.
 the arithmetic average of the highest bid and lowest ask) and more orders
 away from the mid price.
 The gap between the lowest ask and highest bid is called the bid-ask spread.
 A larger gap usually implies a higher friction in the market.
\end_layout

\begin_layout Standard
In general, the dynamics of the limit order book distribution is a very
 challenging problem.
 However, under the assumption that the replicating portfolio hedges a small
 position, we can safely conclude that the trading cost, defined as the
 additional cost paid for buying (or selling, correspondingly) a unit of
 asset with reference to its mid price, can be computed as follows
\begin_inset Formula 
\[
\text{Trading cost (TC)}=S_{\text{ask}}-S_{\text{mid}}=\frac{1}{2}\left(S_{\text{ask}}-S_{\text{bid}}\right)=\frac{1}{2}\text{spread}.
\]

\end_inset

To model the spread, we assume that it scales with the mid price by a factor
 of 
\begin_inset Formula $\epsilon$
\end_inset

, characterizing the friction.
 Thus, the trading cost to buy/sell 
\begin_inset Formula $\Delta u$
\end_inset

 shares of the asset requires a trading cost of
\begin_inset Formula 
\[
\text{TC}\left(\Delta u,S_{\text{mid}}\right)=\frac{\epsilon}{2}S_{\text{mid}}\Delta u.
\]

\end_inset


\end_layout

\begin_layout Section
QLBS: Q-learning Black-Scholes Model
\end_layout

\begin_layout Subsection
A Brief Review
\end_layout

\begin_layout Standard
We quickly introduce the QLBS model proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "halperin2020qlbs"
literal "false"

\end_inset

.
 Consider a sequence of asset prices 
\begin_inset Formula $\left\{ S_{t}\right\} _{t=0,1,\dots}$
\end_inset

, adapted under the filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

, upon which we wish to build an option with payoff function 
\begin_inset Formula $h$
\end_inset

 at the maturity time 
\begin_inset Formula $T$
\end_inset

.
 The option is realized as a hedge portfolio which consists of some shares
 
\begin_inset Formula $u_{t}$
\end_inset

 of the underlying asset and the risk-free deposit 
\begin_inset Formula $B_{t}$
\end_inset

.
 Let 
\begin_inset Formula 
\[
\Pi_{t}:=u_{t}S_{t}+B_{t}
\]

\end_inset

denote the value of the portfolio at time 
\begin_inset Formula $t$
\end_inset

.
 To fulfill the option contract, the holding position is cleared at the
 terminal time 
\begin_inset Formula $T$
\end_inset

 and is fully converted into cash position, i.e.
\begin_inset Formula 
\[
\Pi_{T}=B_{T}=h\left(S_{T}\right).
\]

\end_inset

The deposit position at a particular time is solved via the self-financing
 condition which requires that the instantaneous value of the portfolio
 is kept same before and after the re-hedging operation:
\begin_inset Formula 
\begin{equation}
u_{t}S_{t+1}+e^{r\Delta t}B_{t}=u_{t+1}S_{t+1}+B_{t+1}\label{eq:self-financing}
\end{equation}

\end_inset

where 
\begin_inset Formula $r$
\end_inset

 stands for the risk-free interest rate.
 Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:self-financing"

\end_inset

 can be used to solve how much money is needed to cover future trading activitie
s.
\end_layout

\begin_layout Standard
A state is defined as a pair of an integer and a real number 
\begin_inset Formula $\left(t,X_{t}\right)$
\end_inset

, where 
\begin_inset Formula 
\[
X_{t}=-\left(\mu-\frac{\sigma^{2}}{2}\right)t+\log S_{t}
\]

\end_inset

is the compensated logarithm price.
 The action space is the real number, indicating how much shares are hedged
 at a particular time.
 A policy is a mapping from the state space to the action space, i.e.
\begin_inset Formula 
\[
\pi:\left(t,X_{t}\right)\mapsto a_{t}.
\]

\end_inset

Notice that we use 
\begin_inset Formula $a_{t}$
\end_inset

 for the log-processed input 
\begin_inset Formula $X_{t}$
\end_inset

 while 
\begin_inset Formula $u_{t}$
\end_inset

 for the asset price 
\begin_inset Formula $S_{t}$
\end_inset

 in normal scale.
 The policy may depend on other macro factors, e.g.
 interest rate 
\begin_inset Formula $r$
\end_inset

, volatility 
\begin_inset Formula $\sigma$
\end_inset

, total maturity time 
\begin_inset Formula $T$
\end_inset

, the strike price 
\begin_inset Formula $K$
\end_inset

 if the option is of call/put type.
\end_layout

\begin_layout Standard
The reward function is derived from the Bellman's optimality equation where
 we define the value function in the first place.
 The idea is to minimize the money needed to initiate the hedge portfolio
 as well as to minimize the volatility throughout the trading periods.
 Given a hedging strategy 
\begin_inset Formula $\pi$
\end_inset

, the value function is defined as
\begin_inset Formula 
\begin{equation}
V_{t}^{\pi}\left(X_{t}\right)=\bE^{\pi}\left[-\Pi_{t}\left(X_{t}\right)-\lambda\sum_{\tau=t}^{T}e^{-r\left(\tau-t\right)}\text{Var}\left[\Pi_{\tau}\left(X_{\tau}\right)|\cF_{\tau}\right]|\cF_{t}\right]\label{eq:qlbs-value-function}
\end{equation}

\end_inset

where 
\begin_inset Formula $\lambda$
\end_inset

 is the risk aversion factor.
 The reward function can be derived by matching the corresponding terms
 in the Bellman's equation:
\begin_inset Formula 
\[
R_{t}\left(X_{t},a_{t},X_{t+1}\right):=\gamma a_{t}\Delta S_{t}-\lambda\text{Var}\left[\Pi_{t}|\cF_{t}\right]
\]

\end_inset

where 
\begin_inset Formula $\gamma:=e^{-r\Delta t}$
\end_inset

 is the discounting factor.
 The connection between the value function and option pricing is that the
 option price is given by the minus optimal Q-function.
\end_layout

\begin_layout Subsection
Make QLBS Interactive
\end_layout

\begin_layout Standard
Elegant as the vanilla QLBS approach, it is not a true RL problem since
 the optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

 is solved analytically without any reinforcement learning techniques.
 In fact, the author derives the Bellman's equation for the optimal Q-function
 from Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-value-function"

\end_inset


\begin_inset Formula 
\begin{equation}
Q_{t}^{*}\left(X_{t},a_{t}\right)=\gamma\bE_{t}\left[Q_{t+1}\left(X_{t+1},a_{t+1}^{*}\right)+a_{t}\Delta S_{t}\right]-\lambda\gamma^{2}\bE_{t}\left[\widehat{\Pi}_{t+1}^{2}-2a_{t}\widehat{\Pi}_{t+1}\Delta\widehat{S}_{t}+a_{t}^{2}\left(\Delta\widehat{S}_{t}\right)^{2}\right]\label{eq:qlbs-optimal-Q-function}
\end{equation}

\end_inset

which admits the optimal policy in closed form since Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-optimal-Q-function"

\end_inset

 is a quadratic function in 
\begin_inset Formula $a_{t}$
\end_inset

.
 Such direct approach is feasible if provided with abundant data on the
 correlation structure of the portofolio value 
\begin_inset Formula $\widehat{\Pi}$
\end_inset

 and the stock price change 
\begin_inset Formula $\Delta\widehat{S}$
\end_inset

, but it fails to generalize beyond this simple setting.
 Besides, the portofolio value process 
\begin_inset Formula $\Pi_{t}$
\end_inset

 is in general non-adapted due to how the self-financing condition (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:self-financing"

\end_inset

) works.
 
\end_layout

\begin_layout Standard
To deal with the aforementioned issues, we propose a modified QLBS model
 which is
\end_layout

\begin_layout Enumerate
fully adapted with respect to the given filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

,
\end_layout

\begin_layout Enumerate
compatible with the transaction cost proposed in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Trading-Cost"

\end_inset

, and
\end_layout

\begin_layout Enumerate
works well with value-based or policy-based learning algorithms.
\end_layout

\begin_layout Standard
We start by modifying the value function.
 A first problem lies in the fact that the reward are not homogeneous over
 time.
 Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-value-function"

\end_inset

 assigns the (negative) cashflow with a risk part as the reward, but the
 terminal step gets the option payoff which is in general much larger than
 the previous steps.
 Technically speaking, an agent could notice this heterogenity since the
 time 
\begin_inset Formula $t$
\end_inset

 aligns with the terminal time 
\begin_inset Formula $T$
\end_inset

, but it is rather difficult in practice to figure out this situation.
 Thus, we propose the modified value function 
\begin_inset Formula 
\begin{equation}
V_{t}^{\pi}\left(X_{t}\right)=\bE_{t}^{\pi}\left[-\left(1-\frac{t}{T}\right)\Pi_{t}\left(X_{t}\right)-\lambda\sum_{\tau=t}^{T}\gamma^{\tau-t}\sqrt{\text{Var}\left[\Pi_{\tau}\left(X_{\tau}\right)\right]}\right].\label{eq:qlbs-modified-V}
\end{equation}

\end_inset

Our improvement is two folds:
\end_layout

\begin_layout Enumerate
We weight the portfolio term by a diminishing factor 
\begin_inset Formula $\left(1-\frac{t}{T}\right)$
\end_inset

.
 This factor does not impact the starting estimate at 
\begin_inset Formula $t=0$
\end_inset

 and it fully vanishes at 
\begin_inset Formula $t=T$
\end_inset

.
 We point out that introducing this factor will break the temporal symmetry
 so that the intermediate estimate does not correspond to option pricing
 from the intermediate time steps.
\end_layout

\begin_layout Enumerate
We take a square root of the variance terms so that they turn into standard
 deviation.
 This helps to keep the value estimate dimensionless and robust.
\end_layout

\begin_layout Standard
The reward function changes accordingly to
\begin_inset Formula 
\begin{equation}
R_{t+1}\left(X_{t},a_{t}\right)=\bE_{t}^{\pi}\left[-\left(1-\frac{t}{T}\right)\Pi_{t}\left(X_{t}\right)+\left(1-\frac{t+1}{T}\right)\Pi_{t}\left(X_{t}\right)-\lambda\sqrt{\text{Var}\left[\Pi_{t}\left(X_{t}\right)\right]}\right]\label{eq:qlbs-modified-reward}
\end{equation}

\end_inset

with action at time 
\begin_inset Formula $t$
\end_inset

 to be 
\begin_inset Formula $a_{t}$
\end_inset

 and remaining actions following the current policy 
\begin_inset Formula $\pi$
\end_inset

.
 Considering the effect of transaction costs, the portfolio value process
 
\begin_inset Formula $\Pi_{t}$
\end_inset

 is calculated backwards from the modified self-financing condition
\begin_inset Formula 
\begin{equation}
e^{r\Delta t}\left(\Pi_{t}-u_{t}S_{t}\right)+u_{t}S_{t+1}=\Pi_{t+1}+\text{TC}\left(u_{t+1}-u_{t},S_{t+1}\right)\label{eq:qlbs-modified-self-financing}
\end{equation}

\end_inset

with terminal condition 
\begin_inset Formula $\Pi_{T}=h\left(S_{T}\right)$
\end_inset

 unchanged.
 We point out that we wrap the cashflow part with an conditional expectation
 at time 
\begin_inset Formula $t$
\end_inset

 so that we don't run into adaptedness issues.
 We briefly illustrate the modified QLBS model in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-illustration"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs illustration.png
	width 60line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A brief illustration on the modified QLBS environment.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-illustration"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Specification
\end_layout

\begin_layout Standard
In this section, we describe how to set-up the QLBS environment and the
 necessary numerical procedures.
\end_layout

\begin_layout Subsubsection
Environment 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Environment"

\end_inset


\end_layout

\begin_layout Standard
The environment is responsible for keeping track of the asset price and
 portfolio value based on the actions provided by the agent.
 With a given set of parameters 
\begin_inset Formula $r,\mu,\sigma,T$
\end_inset

, the asset prices are a set of geometrical brownian motion paths 
\begin_inset Formula $\left\{ S_{t}\right\} $
\end_inset

 which solves
\begin_inset Formula 
\[
\d{S_{t}}=\mu S_{t}\d t+\sigma S_{t}\d{W_{t}}
\]

\end_inset

where 
\begin_inset Formula $W_{t}$
\end_inset

 refers to the standard brownian motion according to the filtration 
\begin_inset Formula $\left\{ \cF_{t}\right\} $
\end_inset

.
 At each time step 
\begin_inset Formula $t$
\end_inset

, the normalized price 
\begin_inset Formula $X_{t}=-\left(\mu-\frac{\sigma^{2}}{2}\right)t+\log S_{t}$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 are provided to the agent, waiting for the response of the hedge position
 
\begin_inset Formula $a_{t}$
\end_inset

.
 Then, the reward 
\begin_inset Formula $R_{t+1}$
\end_inset

, as a conditional expectation specified in Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-reward"

\end_inset

, is computed empirically by averaging samples from a fixed number of additional
 trajectories under the current policy.
 At the beginning of each episode, the parameters are adjusted in a random
 fashion to help the agent explore different settings and help avoid overfitting
; the adjustment obeys a Poisson process with intensity 
\begin_inset Formula $\varUpsilon$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Agent Parametrization 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Agent-Parametrization"

\end_inset


\end_layout

\begin_layout Standard
The agent is fully responsible for determining the hedge position 
\begin_inset Formula $a_{t}$
\end_inset

 under a given normalized price at each time step.
 The policy 
\begin_inset Formula $\pi$
\end_inset

, whether stochastic or deterministic, depends on these input variables
 as well as the environment parameters
\begin_inset Formula 
\[
a_{t}\sim\pi\left(X_{t},t;r,\mu,\sigma,T,K,\lambda\right).
\]

\end_inset

In pratice, we prefer a stochastic policy since it encourages exploration
 which is helpful to excape local minima.
 To further simplify the sampling procedure, we restrict our policy spaces
 to Gaussian distribution where the agent determines the mean and standard
 deviation, i.e.
\begin_inset Formula 
\[
\pi=\cN\left(\mu_{\pi},\sigma_{\pi}\right)
\]

\end_inset

(where the subscripts 
\begin_inset Formula $\pi$
\end_inset

 are used to distinguish these parameters from the environmental ones).
 The statistics 
\begin_inset Formula $\mu_{\pi}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\pi}$
\end_inset

 are parametrized by two separate neural networks with the Resnet 
\begin_inset CommandInset citation
LatexCommand cite
key "he2016deep"
literal "false"

\end_inset

 skip-connection structure.
 The Resnet structure is composed of three parts:
\end_layout

\begin_layout Enumerate
Pre-processing 
\begin_inset Formula $T_{\text{lift}}\left(x\right):=\Xi\left(w_{\text{lift}}^{T}x+b_{\text{lift}}\right)$
\end_inset

, that lifts the (8-dimensional) input to the latent dimension by an affine
 transform and an activation funciton 
\begin_inset Formula $\Xi$
\end_inset

;
\end_layout

\begin_layout Enumerate
Chain of transforms 
\begin_inset Formula $\left\{ T^{\left(k\right)}\right\} $
\end_inset

 in a fixed-point iteration style, with each transform combining the identity
 functions and a series of alternating affine transforms 
\begin_inset Formula $\left\{ Z_{l}^{\left(k\right)}\right\} $
\end_inset

 and activation 
\begin_inset Formula $\Xi$
\end_inset

, i.e.
\begin_inset Formula 
\[
T^{\left(k\right)}:=\Xi\circ\left[\text{id}+Z_{n_{k}}^{\left(k\right)}\circ\Xi\circ Z_{n_{k}-1}^{\left(k\right)}\circ\cdots\circ\Xi\circ Z_{1}^{\left(k\right)}\right]
\]

\end_inset


\end_layout

\begin_layout Enumerate
Post-processing 
\begin_inset Formula $T_{\text{project}}\left(x\right):=w_{\text{project}}^{T}x+b_{\text{project}}$
\end_inset

, that projects the latent representations onto the target space.
\end_layout

\begin_layout Standard
Thus, the Resnet realization can be formally written as 
\begin_inset Formula $T_{\text{project}}\circ T^{\left(k\right)}\circ T^{\left(k-1\right)}\circ\cdots\circ T^{\left(1\right)}\circ T_{\text{lift}}$
\end_inset

.
 We include an illustration in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-resnet"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename resnet illustration.png
	width 80line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An illustration on a Resnet-motivated network structure.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-resnet"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Learning Algorithm
\end_layout

\begin_layout Standard
Since the action space 
\begin_inset Formula $a_{t}\in\bR$
\end_inset

 is a continuous space, it is nature to adopt a policy-based method.
 Here we opt-in the classical REINFORCE algorithm with baseline where the
 policy components relies on the value estimator to learn quickly and reliably
 while the value estimator learns from empirical averages of Monte Carlo
 samples.
 The value estimator, parameterized by a neural network, uses the same Resnet
 structure as mentioned in the previous section.
 We refer to 
\begin_inset CommandInset citation
LatexCommand cite
key "sutton2018reinforcement"
literal "false"

\end_inset

 for more details.
\end_layout

\begin_layout Standard
In our implementation, the policy network and the value network have the
 same latent dimension 10 and they are composed of two Resnet blocks with
 two hidden affine transforms.
 We use the Adam optimizers to update the networks, one for each, and the
 learning rate is set to 
\begin_inset Formula $10^{-4}$
\end_inset

 by default.
\end_layout

\begin_layout Subsection
Experiments and Results
\end_layout

\begin_layout Standard
We include a variety of experiments, starting from a demonstration that
 shows the agent learns over time, moving to a comparison with the Black-Scholes
 baseline model, and finally exploring other directions that have a stronger
 connection to finance.
\end_layout

\begin_layout Subsubsection
Demonstration on learning
\begin_inset CommandInset label
LatexCommand label
name "subsec:Demonstration-on-learning"

\end_inset


\end_layout

\begin_layout Standard
We start by showing that the policy gradient algorithm has been correctly
 implemented and the agent learns well over time.
 The environment parameter is set to 
\begin_inset Formula $r=0.01,\sigma=0.1,\mu=0,T=5,K=1,S_{0}=1,\Delta t=1,\lambda\in\left\{ 0,1,2,3\right\} ,\epsilon=0$
\end_inset

 in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Demonstration-on-learning"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Influence-of-risk"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Effect-of-transaction"

\end_inset

 unless otherwise specified.
 As shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp1-learning"

\end_inset

, the episodic return flatterns after training for around 3000 steps.
 It is worth noticing that the cashflow return part 
\begin_inset Formula 
\[
\sum_{t=0}^{T-1}\bE^{\pi}\left[-\left(1-\frac{t}{T}\right)\Pi_{t}\left(X_{t}\right)+\left(1-\frac{t+1}{T}\right)\Pi_{t}\left(X_{t}\right)\right]=\bE^{\pi}\left[\Pi_{0}\left(X_{0}\right)\right]
\]

\end_inset

is slightly smaller for a large risk parameter 
\begin_inset Formula $\lambda$
\end_inset

, implying that the agent learns to make a trade-off between the cash-flow
 and the risk component.
 
\end_layout

\begin_layout Standard
To prepare for learning in a much broader setting, we allow the initial
 price to be adjusted at a given intensity 
\begin_inset Formula $\varUpsilon$
\end_inset

.
 We mark out the episodes that the adjustments take place (Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp1-mutate"

\end_inset

) and it seems that the agent can swiftly adapt to the change.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename qlbs/experiment1/learning-curve.png
	width 45line%

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
No adjustment on environment parameters.
 The shade indicates 95% confidence interval.
 Learning rate set to 
\begin_inset Formula $10^{-4}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp1-learning"

\end_inset


\end_layout

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename qlbs/experiment1/mutate.png
	width 45line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Adjustment parameter (on the initial price) 
\begin_inset Formula $\varUpsilon=0.005$
\end_inset

, indicated by purple lines.
 The shade indicates 95% confidence interval.
 Learning rate set to 
\begin_inset Formula $10^{-4}$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp1-mutate"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Exponentially moving average of episodic return.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Influence of risk parameter 
\begin_inset Formula $\lambda$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "subsec:Influence-of-risk"

\end_inset


\end_layout

\begin_layout Standard
We wish to compare the optimal price under different choices of the risk
 aversion parameter 
\begin_inset Formula $\lambda$
\end_inset

.
 A priori estimate is that a larger 
\begin_inset Formula $\lambda$
\end_inset

 leads to a higher price learnt, since it penalizes improper hedges harder.
 To complete the comparision, we also introduce the vanilla BS hedging strategy
 
\begin_inset Formula $\pi_{\text{BS}}$
\end_inset

 and learns the episodic return using the same neural network parametrization.
 The results are shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp2"

\end_inset

.
 We remind the readers that the option prices are defined as the negative
 value function, so a lower option price implies a larger return learnt.
 As we expect, a large 
\begin_inset Formula $\lambda$
\end_inset

 does lead to a higher price, both under BS policy and under the learnt
 policy.
 In general, the policy learnt by the neural network parametrization is
 a bit sub-optimal compared to the BS policy, but it performs better when
 
\begin_inset Formula $\lambda=0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs/experiment2/option-price.png
	width 75line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Negative episodic returns as option prices.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Effect of transaction cost
\begin_inset CommandInset label
LatexCommand label
name "subsec:Effect-of-transaction"

\end_inset


\end_layout

\begin_layout Standard
Next, we'd like to examine the effect of transaction costs.
 As indicated by the self-financing condition (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-self-financing"

\end_inset

), a higher trading cost can significantly bias the portfolio value process
 and consequently increase the optimal price learnt.
 As shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp3"

\end_inset

, we compare the price and hedge position learnt under different friction
 parameters 
\begin_inset Formula $\epsilon$
\end_inset

.
 In general, a larger 
\begin_inset Formula $\epsilon$
\end_inset

 does lead to a higher price learned (shown in the left half) and the hedge
 position is usually always higher (in the right half).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs/experiment3/price-hedge.png
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Option priced and hedged under different transaction cost parameters 
\begin_inset Formula $\epsilon$
\end_inset

.
 
\begin_inset Formula $\lambda$
\end_inset

 is set to 0.5 to introduce some risk concerns.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp3"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Generalization power
\end_layout

\begin_layout Standard
As a concluding setting, we wish to parametrize the policy and baseline
 networks on not only the states but also the environment parameters and
 examine the generalization power.
 We pick two set of parameters: 
\begin_inset Formula $r=0.01,\mu=0,\sigma=0.1,\lambda=0.5,\epsilon=0$
\end_inset

 under the first condition while 
\begin_inset Formula $r=0.02,\mu=0.1,\sigma=0.2,\lambda=1.5,\epsilon=0.1$
\end_inset

 under the second condition.
 We train a policy and baseline under these two conditions mixed, i.e.
 they have the same probability of being presented, with the switching process
 being a Poisson process.
 After training for a while, we refine the agent to learn a third condition
 where the parameters are set to the arithmic average of the given two condition
s.
 As shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:qlbs-exp4"

\end_inset

, we examine the hedging strategy of the agent before and after fine tuning
 to the third average condition.
 The result, that the fine tuning does a great improvement, is not surprising
 since we don't expect a lot of generalization power.
 However, one could expect such generalization given enough amount of training
 time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename qlbs/experiment4/price-hedge.png
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Option priced and hedged under different conditions.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:qlbs-exp4"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
RLOP: Replication Learning of Option Pricing
\end_layout

\begin_layout Standard
In this section, we propose a novel algorithm that prices a call/put option
 via portfolio replication, but this method uses a forward view which is
 fundamentally different from the QLBS approach.
 The idea is simple: the agent manages a portfolio which yields a reward
 at the terminal time based on how accurate the portfolio value is compared
 to the option payoff.
 This naive idea has the problem that the reward is zero for quite a long
 time until the maturity, which is usually not good for shaping the agent's
 behavior.
 To deal with this downside, we propose to group a few options as an ensemble
 so that the agent gets a stream of feedbacks during each episode.
 To be specific, given a (simulated or historical) path of the asset price
 
\begin_inset Formula $\left\{ S_{t}\right\} $
\end_inset

, maturity time 
\begin_inset Formula $T$
\end_inset

, and the payoff function 
\begin_inset Formula $h$
\end_inset

, the agent needs to manages (at most) 
\begin_inset Formula $T$
\end_inset

 portfolios 
\begin_inset Formula $\left\{ \Pi_{t}^{\left(i\right)}\right\} _{i=1}^{T}$
\end_inset

 at the same time, where the 
\begin_inset Formula $k$
\end_inset

-th portfolio replicates the option that terminals at time step 
\begin_inset Formula $k$
\end_inset

.
 In other words, for every 
\begin_inset Formula $i\in\left[t+1,T\right]$
\end_inset

, the agent proposes the hedge position 
\begin_inset Formula $u_{t}^{\left(i\right)}$
\end_inset

 based on the current time step 
\begin_inset Formula $t$
\end_inset

, the asset price 
\begin_inset Formula $S_{t}$
\end_inset

, and the balance of the 
\begin_inset Formula $i$
\end_inset

-th portfolio 
\begin_inset Formula $\Pi_{t}^{\left(i\right)}$
\end_inset

.
 We hold the belief that the agent is able to learn the hedging strategy
 step by step from small 
\begin_inset Formula $t$
\end_inset

 to the terminal time 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Subsection
MDP Formulation
\end_layout

\begin_layout Standard
We now rigorously define the aforementioned problem as a MDP.
 Given a maturity time 
\begin_inset Formula $i$
\end_inset

, the state space consists of tuples containing the time step 
\begin_inset Formula $t$
\end_inset

, the asset price 
\begin_inset Formula $S_{t}$
\end_inset

, and the current portfolio value 
\begin_inset Formula $\Pi_{t}^{\left(i\right)}$
\end_inset

.
 The action space is 
\begin_inset Formula $\bR$
\end_inset

 that contains all possible hedging positions.
 The transitional probability (density) function is defined as
\begin_inset Formula 
\[
p\left(\left(t,S_{t},\Pi_{t}^{\left(i\right)}\right),u_{t}^{\left(i\right)}\to\left(t',S_{t'},\Pi_{t'}^{\left(i\right)}\right),R_{t+1}\right)=\begin{cases}
\delta_{t+1,t'}\delta_{\widetilde{\Pi}_{t+1}^{\left(i\right)},\Pi_{t'}^{\left(i\right)}}\rho\left(S_{t},S_{t'}\right) & t<i\\
0 & t=i
\end{cases}
\]

\end_inset

where 
\end_layout

\begin_layout Itemize
the only admissible state is the terminal state when 
\begin_inset Formula $t>i$
\end_inset

, marking the end of this episode;
\end_layout

\begin_layout Itemize
\begin_inset Formula $\widetilde{\Pi}_{t+1}^{\left(i\right)}$
\end_inset

 refers to the portfolio value determined by the self-financing condition
\begin_inset Formula 
\[
\widetilde{\Pi}_{t+1}^{\left(i\right)}=e^{r\Delta t}\left(\Pi_{t}-u_{t}^{\left(i\right)}S_{t}\right)+u_{t}^{\left(i\right)}S_{t+1}-\text{TC}\left(u_{t+1}^{\left(i\right)}-u_{t}^{\left(i\right)},S_{t+1}\right)
\]

\end_inset

(which is the same as Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-self-financing"

\end_inset

);
\end_layout

\begin_layout Itemize
\begin_inset Formula $\rho$
\end_inset

 characterizes the dynamics of the underlying asset, e.g.
 the discrete version geometric brownian motion;
\end_layout

\begin_layout Itemize
\begin_inset Formula $R_{t+1}=0$
\end_inset

 for 
\begin_inset Formula $t+1<i$
\end_inset

 and 
\begin_inset Formula $R_{i}=H\left(h\left(S_{i}\right),\Pi_{i}^{\left(i\right)}\right)$
\end_inset

 where 
\begin_inset Formula $H$
\end_inset

 is a given penalty function that measures how accurate the portfolio value
 
\begin_inset Formula $\Pi_{i}^{\left(i\right)}$
\end_inset

 mimics the option payoff 
\begin_inset Formula $h\left(S_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
As we mentioned in the previous paragraph, we stack a few pricing tasks
 together so that the agent gets a reward from the portfolio that recently
 terminates.
 For a 
\begin_inset Formula $T$
\end_inset

-term option, we combine 
\begin_inset Formula $T$
\end_inset

 tasks together where the agent is required to learn to price not only the
 
\begin_inset Formula $T$
\end_inset

-term one but all 
\begin_inset Formula $t$
\end_inset

-term ones for 
\begin_inset Formula $t<T$
\end_inset

.
 We briefly illustrate the stucture of RLOP in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rlop-illustration"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename rlop illustration.png
	width 60line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A brief illustration on the RLOP environment.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-illustration"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Specification
\end_layout

\begin_layout Standard
In this section, we describe how to set-up the RLOP environment and the
 necessary numerical procedures.
\end_layout

\begin_layout Subsubsection
Environment
\end_layout

\begin_layout Standard
As in the QLBS model, the environment samples a family of asset price trajectori
es and maintains the portfolio value process.
 The asset value is assumed to obey the geometrical brownian motion, as
 in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Environment"

\end_inset

.
 We also allows the environment parameters to change at the beginning of
 each episode according to a Poisson process with intensity 
\begin_inset Formula $\varUpsilon$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Agent Parametrization and Learning Algorithm
\end_layout

\begin_layout Standard
The agent, i.e.
 the hedging strategy is parametrized by a Resnet-motivated structure, same
 in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Agent-Parametrization"

\end_inset

.
 The REINFORCE algorithm is adopted to train the agent.
 One caveat is that there is no need to use a baseline as in the QLBS model,
 since in this case the reward function is for penalty rather than learning
 the optimal price.
\end_layout

\begin_layout Subsection
Experiments and Results
\end_layout

\begin_layout Standard
We include a variety of experiments, including a demonstration that shows
 the agent learns over time, a comparison with the Black-Scholes baseline
 model, and a another one on the effect of trading costs.
\end_layout

\begin_layout Subsubsection
Demonstration on learning
\begin_inset CommandInset label
LatexCommand label
name "subsec:Demonstration-on-learning-rlop"

\end_inset


\end_layout

\begin_layout Standard
We use a parameter setting similar to Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Demonstration-on-learning"

\end_inset

 where 
\begin_inset Formula $r=0.01,\sigma=0.1,\mu=0,T=5,K=1,S_{0}=1,\Delta t=1$
\end_inset

.
 The agent is trained under no transaction cost with a fixed initial asset
 value and the training curve is flat after around 8000 steps, shown in
 Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rlop-exp1-learns"

\end_inset

.
 Similar to the QLBS setting, we introduce adjustment to the parameters
 (i.e.
 the initial asset value in the experiments to follow) by a Poisson process.
 We mark where the adjustment occurs in the bottom half of the figure by
 purple triangles.
\end_layout

\begin_layout Standard
We also compare the optimal hedging strategy learnt with the BS predictions.
 In general, the hedging curve should have a greater slope for a smaller
 remaining time which implies that the price of the option is more sensitive
 to the underlying asset price.
 As expected, both the learnt position and the BS position show a greater
 slope when approaching the terminal time.
 The one learnt by our RL agent is slightly insensitve to the time variable.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename rlop/experiment1/learning-curve.png
	width 47line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Episodic return improves during training.
 Top figure: no adjustment on the initial price 
\begin_inset Formula $S_{0}$
\end_inset

; bottom figure: the initial price is adjusted via a Poisson process.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp1-learns"

\end_inset


\end_layout

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename rlop/experiment1/hedge_position.png
	width 47line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Optimal hedge position learnt compared with BS predictions (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bs-hedge"

\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp1-hedge"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Demonstration the RLOP model learning curve and a comparison to the BS predictio
ns.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Effect of transaction cost
\begin_inset CommandInset label
LatexCommand label
name "subsec:Effect-of-transaction-rlop"

\end_inset


\end_layout

\begin_layout Standard
We proceed to measure how the transaction cost can effect the optimal hedging
 strategy.
 We use the same setting as laid out in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Effect-of-transaction"

\end_inset

 where 
\begin_inset Formula $\epsilon$
\end_inset

 is the friction parameter.
 We plot the optimal hedging position learnt under different 
\begin_inset Formula $\epsilon$
\end_inset

 in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rlop-exp2"

\end_inset

.
 The general trend is that a large 
\begin_inset Formula $\epsilon$
\end_inset

 discourages hedging amounts, since it leads to a higher trading cost on
 average.
 A higher friction also leads to a larger distinction between positions
 at different times, which implies that in an illuquid market, we need to
 value the time variable more than in a liquid one.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename rlop/experiment2/hedge_position.png
	width 100line%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Optimal hedging strategy learnt under different transaction costs.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:rlop-exp2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion and Discussion
\end_layout

\begin_layout Standard
In this project, we construct the modified QLBS and RLOP as two mathematically
 sound environments that enable RL methods in the field of option pricing.
 We specify the environment details as well as how the agent is parametrized
 and trained; the source code can be found at the repo
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/owen8877/RLOP"
literal "true"

\end_inset


\end_layout

\end_inset

.
 We provide a few numerical experiments which compare the learned optimal
 hedging against the BS predictions.
 We also study how the factors affect the optimal price and position.
 We hope that this work can inspire and motivate more research into the
 cross-discipline study between finance and machine learning.
\end_layout

\begin_layout Standard
We would like to discuss a few possible future directions, starting from
 the financial aspects.
 We focus on European call options in this report, but in general, it shall
 also apply to the put option or other options that can only exercise at
 the terminal time.
 It is not clear whether the QLBS or RLOP model can generalize to the American
 or even swing options that are useful in electricity trading.
 Besides, the transaction cost model proposed in Sec.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Trading-Cost"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be greatly improved to allow fine models on the order density, or even
 influence on the mid price.
\end_layout

\begin_layout Standard
There are also a few open questions on the mathematics side.
 It could be beneficial to analyze the theoretical optimal hedging strategy
 in the discrete time model.
 We have provided an analogous to the stochastic integral in the earlier
 sections, which might help in analyzing the temporal discretization error.
 Besides, the modified QLBS reward function (Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:qlbs-modified-V"

\end_inset

) is no longer time translational invariant, so one has to train a new model
 if the maturity time of the option changes.
 It would be a great improvement if one finds an equivalent or a similar
 formulation where the value function is translational invariant while giving
 homogeneous rewards over time.
\end_layout

\begin_layout Standard
We also have a few comments on the learning/sampling algorithms.
 One common drawback of the two methods is that the computational complexity
 scales quadratically as the maturity time 
\begin_inset Formula $T$
\end_inset

.
 The modified QLBS model requires computing a conditional expectation at
 each time step, which is not of constant cost.
 The RLOP model stacks 
\begin_inset Formula $T$
\end_inset

 pricing tasks together, each lasting one more period than the previous
 one, leading to a quadratic cumulative cost.
 Another issue is that we use REINFORCE with baseline as the learning algorithm,
 which can be replaced by the actor-critic (or even the soft version) to
 improve performance.
 More time is appreciated since we have limited time in the project to try
 different possibilities.
\end_layout

\begin_layout Standard
There are controversial conclusions as far as what the numerical experiments
 show.
 In general, the agents learn to price a higher price if the current price
 
\begin_inset Formula $S_{t}$
\end_inset

 is higher, or the risk aversion parameter 
\begin_inset Formula $\lambda$
\end_inset

 (or the friction parameter 
\begin_inset Formula $\epsilon$
\end_inset

) is larger.
 However, the two models predict the optimal hedging strategy differently,
 especially when large friction is anticipated.
 The modified QLBS agent prefers to hedge more than the frictionless case
 while the RLOP agent inclines to hedge less.
 This discrepancy might be the key to understanding the difference between
 these two models and eventually the option pricing problem.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Section*
Video Transcript
\end_layout

\begin_layout Plain Layout
Hello! It's my pleasure to introduce our recent project on RL methods in
 option pricing.
 We aim to build a set of environments that captures the essence of the
 underlying financial insight while enabling machine learning in this regime.
 To begin with, option is a type of derivative contract, that gives the
 holder the right so that one can choose to buy or sell a certain amount
 of asset at a specific time.
 There has been some very successful theories about option pricing, for
 example the celebrated Black-Scholes formula.
 The idea is to replicate the option payoff by managing a hedge portfolio
 which involves the underlying asset and a cash account.
 The Black-Scholes formula produces the optimal hedge position series 
\begin_inset Formula $u_{t}$
\end_inset

 so that the payoff is perfectly replicated at the terminal time.
 However, their theory relies on continuous rehedging and assumes zero trading
 costs, which is either impractical or too harsh.
\end_layout

\begin_layout Plain Layout
In general, option pricing is a quite challenging problem.
 One may fail to notice an improper hedging action until the terminal time,
 let alone taking all the other constraints into account, for example discrete
 time, transaction cost and lack of asset pricing models.
 A recent paper by Igor Halperin in 2017 leverages RL theory to deal with
 these issues where the the price of an option includes a cashflow part
 and a variance part.
 The cashflow part is from Black-Scholes formula, while the variance part
 captures the risk from a variety range of factors.
 Besides, this approach does not assume an explicit model for the underlying
 trajectory, which saves the trouble of learning the pricing model.
 With that being said, the QLBS approach is not a fully interactive RL problem
 since the optimal hedging strategy is solved analytically instead of learning
 from exploration.
\end_layout

\begin_layout Plain Layout
In this work, we modify the reward and value function so that it enables
 learning by policy-based methods.
 The cashflow part is adjusted by the remaining time to the terminal time
 and the risk part uses the standard deviation so that it is invariant under
 scaling.
 The agent, under the actor-critic framework, learns the price of the option
 as the negative optimal value function.
 We adopt the policy gradient method to train the agent via an online fashion.
 Through a few numerical experiments, we show how the risk aversion parameter
 and trading cost could influence the optimal price learnt.
 We also compare the optimal price learnt with the one predicted by the
 Black-Scholes formula to better understand the risk premium.
 More details are included in the report.
\end_layout

\begin_layout Plain Layout
We also propose a novel approach which examines the hedge portfolio and
 the option payoff at the terminal time.
 Since the reward may come in a sparse manner as the agent has no idea how
 it is performing before the terminal time, we stack a class of option replicati
on tasks together so that the agent learns how to price a family of options
 at the same time.
 This framework is more straightforward, but it is harder to incorporate
 the risk concerns.
 We hope that our work can motivate and inspire deeper research of RL in
 the finance field.
 Thank you.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref"
options "siam"

\end_inset


\end_layout

\end_body
\end_document
